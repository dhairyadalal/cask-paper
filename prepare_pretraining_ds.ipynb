{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from ast import literal_eval\n",
    "import json \n",
    "from string import Template\n",
    "import random \n",
    "import tqdm.notebook as tqdm \n",
    "\n",
    "sent_templates = json.loads(open(\"src/resources/sentence_templates.json\").read())\n",
    "kb = pd.read_csv(\"data/generated_knowledge/all_extracted_knowledge.csv\")\n",
    "kb = kb.query(\"causal_system != 'error' or knowledge != 'error' \")\n",
    "kb[\"num_relations\"] = kb[\"knowledge\"].apply(lambda x: len(literal_eval(x)))\n",
    "kb = kb.query(\"num_relations > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearize triples to generate a description\n",
    "updated_rows = []\n",
    "\n",
    "for i, row in tqdm.tqdm(kb.iterrows(), total=len(kb)):\n",
    "\n",
    "    row = row.to_dict()\n",
    "\n",
    "    triples = literal_eval(row[\"knowledge\"])\n",
    "    sents = []\n",
    "    for triple in triples:\n",
    "        triple = [k.strip() for k in triple.split(\";\")]\n",
    "        \n",
    "        if triple[1] not in sent_templates:\n",
    "            continue\n",
    "\n",
    "        template = Template(random.choice(sent_templates[triple[1]]))\n",
    "        generated_sent = template.substitute({\"head\":triple[0], \"tail\": triple[2] })\n",
    "        sents.append(generated_sent)\n",
    "    desc = f\"The causal system of {row['causal_system']}. {' '.join(sents)}\"\n",
    "    \n",
    "    row[\"paragraph\"] = desc\n",
    "    updated_rows.append(row)    \n",
    "\n",
    "df = pd.DataFrame(updated_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate MC questions\n",
    "all_facts = []\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    triples = literal_eval(row[\"knowledge\"])\n",
    "    for triple in triples:\n",
    "        triple = [k.strip() for k in triple.split(\";\")]\n",
    "        try:\n",
    "            all_facts.append({\"head\": triple[0], \"tail\": triple[2], \"relation\": triple[1]})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "facts = pd.DataFrame(all_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts = facts.drop_duplicates()\n",
    "entities = list(set(facts[\"head\"].values) | set(facts[\"tail\"].values))\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import random \n",
    "from string import Template\n",
    "\n",
    "question_templates = json.loads(open(\"src/resources/synthetic_qa_templates.json\").read())\n",
    "\n",
    "synthetic_set = []\n",
    "for i, row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "\n",
    "    triples = literal_eval(row[\"knowledge\"])\n",
    "    for triple in triples:\n",
    "        try:\n",
    "            triple = [k.strip() for k in triple.split(\";\")]\n",
    "            head, relation, tail = triple[0], triple[1], triple[2]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        if triple[1] not in question_templates:\n",
    "            continue\n",
    "\n",
    "        mask_entity = random.choice([\"head\", \"tail\"])\n",
    "        answer_options = {\"head\": head, \"tail\": tail}\n",
    "        question_template = Template(random.choice(question_templates[triple[1]][mask_entity]))\n",
    "\n",
    "        question = question_template.substitute({\"head\": head, \"tail\": tail})\n",
    "        options = random.sample(set(entities).difference(answer_options[mask_entity]), 3)\n",
    "        masked_paragraph = row[\"paragraph\"].replace(answer_options[mask_entity], f\"[MASK]\")\n",
    "\n",
    "        correct_answer = random.choice([\"a\", \"b\", \"c\", \"d\"])\n",
    "        if correct_answer == \"a\":\n",
    "            output = f\"a) {answer_options[mask_entity]}\"\n",
    "            label = \"a\"\n",
    "            options = f\"a) {answer_options[mask_entity]} b) {options[0]} c) {options[1]} d) {options[2]}\"\n",
    "        elif correct_answer == \"b\":\n",
    "            output = f\"b) {answer_options[mask_entity]}\"\n",
    "            label = \"b\"\n",
    "            options = f\"a) {options[0]} b) {answer_options[mask_entity]} c) {options[1]} d) {options[2]}\"\n",
    "        elif correct_answer == \"c\":\n",
    "            output = f\"c) {answer_options[mask_entity]}\"\n",
    "            label = \"c\"\n",
    "            options = f\"a) {options[0]} b) {options[1]} c) {answer_options[mask_entity]} d) {options[2]}\"\n",
    "        elif correct_answer == \"d\":\n",
    "            output = f\"d) {answer_options[mask_entity]}\"\n",
    "            label = \"d\"\n",
    "            options = f\"a) {options[0]} b) {options[1]} c) {options[2]} d) {answer_options[mask_entity]}\"\n",
    "\n",
    "        input = f\"question: {question} options: {options} context: {masked_paragraph}\"\n",
    "\n",
    "        synthetic_set.append(\n",
    "            {\n",
    "                \"uid\": row['uid'],\n",
    "                \"input\": input,\n",
    "                \"output\": output,\n",
    "                \"label\": label,\n",
    "                \"question\": question,\n",
    "                \"options\": options,\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df = pd.DataFrame(synthetic_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in qa_df.sample(5).iterrows():\n",
    "    print(r[\"input\"])\n",
    "    print(r[\"output\"])\n",
    "    print(r[\"label\"])\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df.to_csv(\"data/generated_knowledge/synthetic_qa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"data/generated_knowledge/synthetic_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds  = ds.train_test_split(test_size=0.10, seed=42, shuffle=True)\n",
    "ds.save_to_disk(\"data/pretraining/synthetic_qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare external for synthetic qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datasets \n",
    "\n",
    "sent_templates = {\n",
    "    \"cause-effect\": [\n",
    "        \"$head can lead to $tail.\",\n",
    "        \"sometimes $head can result in $tail.\",\n",
    "        \"$head may cause $tail.\",\n",
    "        \"$tail can sometimes be a consequence of $head.\",\n",
    "        \"due to $head, $tail can occur.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "data = datasets.load_from_disk(\"data/pretraining/external_kb\")\n",
    "\n",
    "train = data[\"train\"].to_pandas()\n",
    "test = data[\"test\"].to_pandas()\n",
    "\n",
    "\n",
    "df = pd.concat([train, test]).dropna().drop_duplicates()\n",
    "entities = list(set(df[\"head\"].tolist() + df[\"tail\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992b8af1b5cd46e89288d65cb470c7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/208081 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "sqa_input, sqa_label, sqa_output = [], [], []\n",
    "\n",
    "for i,row in tqdm.tqdm(df.iterrows(), total=len(df)):\n",
    "    input = row[\"clean_input\"]\n",
    "    \n",
    "    concepts = [row[\"head\"], row[\"tail\"]]\n",
    "    concept_candidate = random.choice(concepts)\n",
    "    \n",
    "    valid_entites = list(set(entities).difference(concepts))\n",
    "\n",
    "    options = random.sample(valid_entites, 3)\n",
    "    correct_answer = random.choice([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "    if correct_answer == \"a\":\n",
    "        output = f\"a) {concept_candidate}\"\n",
    "        label = \"a)\"\n",
    "        options = f\"a) {concept_candidate} b) {options[0]} c) {options[1]} d) {options[2]}\"\n",
    "    elif correct_answer == \"b\":\n",
    "        output = f\"b) {concept_candidate}\"\n",
    "        label = \"b)\"\n",
    "        options = f\"a) {options[0]} b) {concept_candidate} c) {options[1]} d) {options[2]}\"\n",
    "    elif correct_answer == \"c\":\n",
    "        output = f\"c) {concept_candidate}\"\n",
    "        label = \"c)\"\n",
    "        options = f\"a) {options[0]} b) {options[1]} c) {concept_candidate} d) {options[2]}\"\n",
    "    elif correct_answer == \"d\":\n",
    "        output = f\"d) {concept_candidate}\"\n",
    "        label = \"d)\"\n",
    "        options = f\"a) {options[0]} b) {options[1]} c) {options[2]} d) {concept_candidate}\"\n",
    "    \n",
    "    \n",
    "    question_text = input.replace(concept_candidate, \"[MASKED]\")\n",
    "    if concept_candidate == row[\"head\"]:\n",
    "        question_text += \" What is the most plausible cause?\"\n",
    "    else:\n",
    "        question_text += \" What is the most plausible effect?\"\n",
    "        \n",
    "    \n",
    "    final_input = f\"question: {question_text} options: {options}\"\n",
    "    \n",
    "    sqa_input.append(input)\n",
    "    sqa_label.append(label)\n",
    "    sqa_output.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sqa_input\"] = sqa_input\n",
    "df[\"sqa_label\"] = sqa_label\n",
    "df[\"sqa_output\"] = sqa_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds  = ds.train_test_split(test_size=0.10, seed=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns(\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['head', 'relation', 'tail', 'source', 'sentence', 'clean_input', 'mlm_input', 'mlm_label', 'concept_input', 'concept_label', 'sqa_input', 'sqa_label', 'sqa_output', '__index_level_0__'],\n",
       "        num_rows: 187272\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['head', 'relation', 'tail', 'source', 'sentence', 'clean_input', 'mlm_input', 'mlm_label', 'concept_input', 'concept_label', 'sqa_input', 'sqa_label', 'sqa_output', '__index_level_0__'],\n",
       "        num_rows: 20809\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk(\"data/pretraining/external_kb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
