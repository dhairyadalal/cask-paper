{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    set_seed\n",
    ")\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "from src.readers.collators import SimpleCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW \n",
    "import lightning as pl\n",
    "\n",
    "\n",
    "class PeftCALMT5(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, model_alias: str, tokenizer_alias: str):\n",
    "\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_alias)\n",
    "\n",
    "        self.peft_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "            inference_mode=False, \n",
    "            target_modules=[\"q\", \"k\", \"v\"],\n",
    "            r=8, \n",
    "            lora_alpha=32, \n",
    "            lora_dropout=0.5\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_alias)\n",
    "        self.model = get_peft_model(model, self.peft_config)\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        outputs = self.model.forward(**batch, return_dict=True)\n",
    "        loss = outputs[\"loss\"]  \n",
    "        \n",
    "        self.log(\"train_loss\", loss,  prog_bar=True, on_step=True, on_epoch=True)     \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs[\"loss\"]  \n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True) \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=5e-4)\n",
    "        return optimizer\n",
    "\n",
    "tokenizer_alias = \"google/flan-t5-base\"\n",
    "#model_alias = \"google/flan-t5-base\"\n",
    "#model_alias = \"models/pretraining/flan-t5-concept\"\n",
    "\n",
    "ablations_map = [\n",
    "    \"models/pretraining/flan-t5-synthetic-qa-ablation-causal\",\n",
    "    \"models/pretraining/flan-t5-synthetic-qa-ablation-contributory\",\n",
    "    \"models/pretraining/flan-t5-synthetic-qa-ablation-impact\",\n",
    "    \"models/pretraining/flan-t5-synthetic-qa-ablation-intent\",\n",
    "    \"models/pretraining/flan-t5-synthetic-qa-ablation-temporal\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "model_alias = ablations_map[]\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "set_seed(42)\n",
    "model = PeftCALMT5(model_alias, tokenizer_alias)\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.readers.collators import ConceptCollator, SimpleCollator, RandomMLMCollator\n",
    "\n",
    "synthetic_collator = SimpleCollator(tokenizer, {\"input_column\": \"input\", \"output_column\": \"output\"})\n",
    "mlm_collator = RandomMLMCollator(tokenizer)\n",
    "concept_collator = ConceptCollator(tokenizer)\n",
    "pretrain_ds = load_from_disk(\"data/pretraining/synthetic-qa\")\n",
    "\n",
    "pretrain_train = pretrain_ds[\"train\"]\n",
    "pretrain_train_dl = DataLoader(\n",
    "    pretrain_train,\n",
    "    batch_size=32, \n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=synthetic_collator\n",
    ")\n",
    "\n",
    "pretrain_eval = pretrain_ds[\"test\"]\n",
    "pretrain_eval_dl = DataLoader(\n",
    "    pretrain_train,\n",
    "    batch_size=32, \n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=synthetic_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "  max_epochs=3,\n",
    "  devices=1, \n",
    "  accelerator=\"gpu\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, pretrain_train_dl, pretrain_eval_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = model.model\n",
    "merged = peft_model.merge_and_unload()\n",
    "merged.save_pretrained(\"models/pretraining/flan-t5-concept-3-epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = SimpleCollator(tokenizer, {\"input_column\": \"input\", \"output_column\": \"output_text\"})\n",
    "ds = load_from_disk(\"data/calm-bench/datasets/copa\")\n",
    "train_ds = ds.filter(lambda x: x['split'] == 'train')\n",
    "train_dl = DataLoader(\n",
    "    train_ds, \n",
    "    batch_size=32, \n",
    "    pin_memory=True, \n",
    "    shuffle=True,\n",
    "    num_workers=4, \n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "val_ds = ds.filter(lambda x: x['split'] == 'val')\n",
    "val_dl = DataLoader(\n",
    "    val_ds, \n",
    "    batch_size=16, \n",
    "    pin_memory=True, \n",
    "    shuffle=False,\n",
    "    num_workers=4, \n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "\n",
    "test_ds = ds.filter(lambda x: x['split'] == 'test')\n",
    "test_dl = DataLoader(\n",
    "    test_ds, \n",
    "    batch_size=32, \n",
    "    pin_memory=True, \n",
    "    shuffle=False,\n",
    "    num_workers=4, \n",
    "    \n",
    "    collate_fn=collator \n",
    ")\n",
    "\n",
    "test_df = test_ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                  | Params\n",
      "------------------------------------------------\n",
      "0 | model | PeftModelForSeq2SeqLM | 248 M \n",
      "------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "247 M     Non-trainable params\n",
      "248 M     Total params\n",
      "995.620   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f972a3ffc7034655a1a67e13bf9d4dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nlp-env/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce8bc81bb024a7890edb33e83562126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d56aff19144bc2a0168ea7dbd9e8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b1c9a50c3d4511851b125baf9f1320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664925fb68243ebaece056a4014572f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "  max_epochs=3,\n",
    "  devices=1, \n",
    "  accelerator=\"gpu\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choice(input_string):\n",
    "    input_string = input_string.strip().lower()\n",
    "    \n",
    "    \n",
    "    if input_string.startswith('a') or input_string.startswith('a)'):\n",
    "        return 'a'\n",
    "    elif input_string.startswith('b') or input_string.startswith('b)'):\n",
    "        return 'b'\n",
    "    elif input_string.startswith('c)') or input_string.startswith('c'):\n",
    "        return 'c'\n",
    "    elif input_string.startswith('d)') or input_string.startswith('d'):         \n",
    "        return 'd'\n",
    "    else:\n",
    "        return 'a'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Wiqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.model\n",
    "predictor.eval()\n",
    "predictor = predictor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tqdm.notebook as tqdm\n",
    "from ast import literal_eval\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "test = pd.read_csv(\"data/calm-bench/tasks/wiqa-updated-test2.csv\")\n",
    "\n",
    "\n",
    "rows = []\n",
    "for i, row in tqdm.tqdm(test.iterrows()):\n",
    "\n",
    "    try:\n",
    "        steps = literal_eval(row[\"question_para_step\"])\n",
    "        context = [\". \".join(tup) for tup in zip(np.arange(1, len(steps)+1).astype(str), steps) ]\n",
    "        context = \" \".join(context)\n",
    "\n",
    "        question = row[\"question\"]\n",
    "        if question[-1] != \"?\":\n",
    "            question += \"?\"\n",
    "        \n",
    "        input_ = f\"context: {context}\\nquestion: {question}\\noptions: a) more b) less c) no effect\".lower()\n",
    "        output_text = f\"a) more\" if row[\"answer_label\"] == \"more\" else f\"b) less\" if row[\"answer_label\"] == \"less\" else f\"c) no effect\"\n",
    "        output_label = \"a\" if row[\"updated_answer\"] == \"more\" else \"b\" if row[\"updated_answer\"] == \"less\" else \"c\"\n",
    "\n",
    "\n",
    "        ii = tokenizer.encode(input_, return_tensors=\"pt\").to(\"cuda\")\n",
    "        pred = predictor.generate(input_ids = ii)\n",
    "        pred = tokenizer.batch_decode(pred, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "        pred_label = get_choice(pred)\n",
    "\n",
    "        r = row.to_dict()\n",
    "        r[\"pred\"] = pred\n",
    "        r[\"pred_label\"] = pred_label\n",
    "        r[\"is_correct\"] = int(pred_label == output_label)\n",
    "\n",
    "        rows.append(r)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "p = pd.DataFrame(rows)\n",
    "p[\"is_correct\"].mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/nlp-env/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictor = model.model\n",
    "predictor.eval()\n",
    "predictor = predictor.to(\"cuda\")\n",
    "\n",
    "all_preds = []\n",
    "for batch in test_dl:\n",
    "\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "    preds = predictor.generate(**batch)\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    all_preds.extend(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"preds\"] = all_preds\n",
    "test_df[\"choice\"] = test_df[\"preds\"].apply(get_choice)\n",
    "test_df[\"is_correct\"] = test_df.apply(lambda x: int(x[\"preds\"] == x[\"output_text\"]), axis=1)\n",
    "\n",
    "test_df[\"is_correct\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
