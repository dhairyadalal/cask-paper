{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare KB for Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import Template\n",
    "\n",
    "kb = pd.read_csv(\"data/generated_knowledge/all-knowledge.csv\")\n",
    "\n",
    "doc_template = Template(\n",
    "    \"$title\\nSummary: $summary\\nDescription: $context\\n\" \n",
    ")\n",
    "\n",
    "documents = []\n",
    "for i, row in kb.iterrows():\n",
    "    doc = doc_template.substitute(\n",
    "        {\n",
    "            \"title\": row[\"title\"],\n",
    "            \"summary\": row[\"description\"],\n",
    "            \"context\": row[\"linearized_paragraph\"]\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "embedder = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"multi-qa-mpnet-base-dot-v1\"\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"kb-cache/\")\n",
    "db = client.create_collection(\n",
    "    name=\"causal-kb\",\n",
    "    embedding_function=embedder,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"ip\",\n",
    "        \"embedding\": \"multi-qa-mpnet-base-dot-v1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.add(\n",
    "    documents=documents,\n",
    "    metadatas=kb.to_dict(orient=\"records\"),\n",
    "    ids=kb[\"uid\"].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline\n",
    "\n",
    "### LLM Query Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch \n",
    "from openai import OpenAI\n",
    "import re\n",
    "from string import Template\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "class QueryEngine:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_alias: str,\n",
    "            template: Template,\n",
    "            quant_config: dict = {}, \n",
    "        ):\n",
    "        self.model_alias = model_alias\n",
    "        key = \"\"\n",
    "\n",
    "        if \"gpt\" not in model_alias:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_alias,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_alias,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=\"auto\",\n",
    "                quantization_config=quant_config\n",
    "            )\n",
    "        else:\n",
    "            self.client = OpenAI(\n",
    "                api_key=key\n",
    "            )\n",
    "            \n",
    "        self.prompt_template = template\n",
    "    \n",
    "    def extract_output(self, text: str) -> str:\n",
    "        text = text.lower().strip()\n",
    "        \n",
    "        # Define the regex pattern to capture the output\n",
    "        pattern = re.compile(r'output:\\s*(.*)', re.IGNORECASE)\n",
    "\n",
    "        # Search for the pattern in the text\n",
    "        match = pattern.search(text)\n",
    "\n",
    "        # Extract and return the output, if a match is found\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "\n",
    "        # Return an empty string if no match is found\n",
    "        return \"\"\n",
    "\n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1))\n",
    "    def query_gpt(self, prompt: str) -> str:\n",
    "        prompt = self.prompt_template.safe_substitute({\"input\": prompt})\n",
    "        response = self.client.chat.completions.create(\n",
    "            model = \"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert in causal reasoning, logical reasoning, and commonsense question-answering. Do not provide an intro or concluding remarks in your response. Be as concise as you can be when responding. \"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=1,\n",
    "            max_tokens=20,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    \n",
    "    def query_llm(self, text: str) -> str:\n",
    "        prompt = self.prompt_template.safe_substitute({\"input\": text})\n",
    "        encoded_prompt = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        #with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "        output = self.model.generate(**encoded_prompt, max_new_tokens=20)\n",
    "        output_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        output_text = output_text.split(\"[\\INST]\")[1]\n",
    "        #output_text = output_text.split(\"[/INST]\")[1]\n",
    "        return self.extract_output(output_text)\n",
    "    \n",
    "    def query(self, query: str):\n",
    "        if \"gpt\" in self.model_alias:\n",
    "            return self.query_gpt(query)\n",
    "        return self.query_llm(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_template = Template(\n",
    "    \"Instruct: Answer the question provided the scenario below. Do not provide an intro or concluding remarks in your response. Do not provide an explanation. Just provide an answer. For multiple-choice return the letter and answer only. $input\\nOutput:\"\n",
    ")\n",
    "\n",
    "gpt_template = Template(\n",
    "    'Answer the question below. Do not provide an explanation. Provide both the letter and answer option. Use the prefix \"output:\" and then provide the answer. \\n $input'\n",
    ")\n",
    "\n",
    "gpt_template_ropes = Template(\n",
    "\"\"\"\n",
    "Answer the question below. Do not provide an explanation. Just provide the answer and nothing more.\n",
    "Example:\n",
    "Sitution: Bearland had a small population, while deerland had a large population. Both peoples lived mostly off the land.\n",
    "Question: which country had less food per individual?\n",
    "Output: bearland\n",
    "\n",
    "Scenario:\n",
    "$input\n",
    "Output:\n",
    "\"\"\"\n",
    ")\n",
    "mistral_template = Template(\n",
    "    \"<s>[INST]Answer the question provided the scenario below. Do not provide an intro or concluding remarks in your response. Do not provide an explanation. Just provide an answer. For multiple-choice return the letter and answer only. $input\\n[/INST]\\nOutput:\"\n",
    ") \n",
    "mistral_template_ropes = Template(\n",
    "     \"\"\"<s>[INST]Answer the question below. Do not provide an explanation.\\nExample:\\nsituation: deerland had a small population, while bearland had a large population. both peoples lived mostly off the land.\\n question: which country had less food per individual?\\nOutput: bearland\\nScenario:\\n$input\\n[/INST]Output:\"\"\"\n",
    ")\n",
    "\n",
    "llama_template = Template(\n",
    "\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are an expert in causal reasoning, logical reasoning, and commonsense question-answering. Do not provide an intro or concluding remarks in your response.  Be as concise as you can be when responding. \n",
    "<</SYS>>\n",
    "Task: \n",
    "Answer the question provided the scenario below. Do not provide an intro or concluding remarks in your response. Do not provide an explanation. Just provide an answer. For multiple-choice return the correct answer.\\n\n",
    "\n",
    "Example:\n",
    "What is the capital of France?\n",
    "Options: \n",
    "a) Paris b) London c) Berlin d) Rome\n",
    "Output:\n",
    "a) Paris\n",
    "\n",
    "Task Input:\n",
    "$input\n",
    "[/INST]    \n",
    "Output:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "llama_ropes_template = Template(\n",
    "\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are an expert in causal reasoning, logical reasoning, and commonsense question-answering. Do not provide an intro or concluding remarks in your response.  Be as concise as you can be when responding. \n",
    "<</SYS>>\n",
    "Answer the question below. Do not provide an explanation.\n",
    "\n",
    "Example:\n",
    "situation: \n",
    "deerland had a small population, while bearland had a large population. both peoples lived mostly off the land.\\\n",
    "question: which country had less food per individual?\n",
    "Output: bearland\n",
    "\n",
    "Scenario:\n",
    "$input:\n",
    "[/INST]    \n",
    "Output:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "#alias = \"microsoft/phi-2\"\n",
    "alias = \"gpt-3.5-turbo\"\n",
    "#alias = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#alias = \"NousResearch/Llama-2-13b-chat-hf\"\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   #bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "engine = QueryEngine(alias, mistral_template, nf4_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choice(input_string):\n",
    "    input_string = input_string.strip().lower()\n",
    "    \n",
    "    if input_string.startswith('a') or input_string.startswith('a)'):\n",
    "        return 'a'\n",
    "    elif input_string.startswith('b') or input_string.startswith('b)'):\n",
    "        return 'b'\n",
    "    elif input_string.startswith('c)') or input_string.startswith('c'):\n",
    "        return 'c'\n",
    "    elif input_string.startswith('d)') or input_string.startswith('d'):         \n",
    "        return 'd'\n",
    "    else:\n",
    "        return 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4746a87c13a54a09b589d4562391dd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chromadb\n",
    "from datasets import load_from_disk\n",
    "from tqdm.notebook import tqdm \n",
    "from chromadb.utils import embedding_functions\n",
    "import pandas as pd \n",
    "from transformers.utils import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"kb-cache/\")\n",
    "embedder = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"multi-qa-mpnet-base-dot-v1\"\n",
    ")\n",
    "db = client.get_collection(\"causal-kb\", embedding_function=embedder)\n",
    "\n",
    "tasks = [\n",
    "   #\"copa\"\n",
    "   #\"ropes\", \n",
    "   #\"wiqa\"\n",
    "   #\"ecare\", \"anli\", \n",
    "   #\"cosmosqa\", \n",
    "   \"ropes\", \n",
    "   #\"wiqa\"\n",
    "]\n",
    "log_path = \"experiment_results/gpt-3.5\"\n",
    "for task in tasks:\n",
    "    ds = load_from_disk(f\"data/calm-bench/datasets/{task}\")\n",
    "    train = ds.filter(lambda x: x[\"split\"] == \"train\") \n",
    "    test = ds.filter(lambda x: x[\"split\"] == \"test\")    \n",
    "\n",
    "    updated_rows = []\n",
    "    for i, row in tqdm(enumerate(test), total=len(test)):\n",
    "        \n",
    "        #example = f\"Context:\\n{row['context']}\\nQuestion:\\n{row['question']}\\nOptions: a) {row['option1']} b) {row['option2']} c) {row['option3']} d) {row['option4']}\"\n",
    "        example = f\"Background:\\n{row['background']}\\nSituation:\\n{row['situation']}\\nQuestion: {row['question']}\"\n",
    "\n",
    "        baseline_pred = engine.query(example)\n",
    "        row[\"baseline_pred\"] = baseline_pred\n",
    "        \n",
    "        # Generate knowledge-augmented prediction\n",
    "        top_context = db.query(query_texts=[row[\"input\"]], n_results=1)[\"documents\"][0][0]\n",
    "        prompt_with_context = f\"Background Knowledge:\\n{top_context}\\n{example}\"\n",
    "        know_pred = engine.query(prompt_with_context)\n",
    "        row[\"know_pred\"] = know_pred\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        if task != \"ropes\":\n",
    "            row[\"baseline_choice\"] = get_choice(baseline_pred)\n",
    "            row[\"baseline_is_correct\"] = int(row[\"baseline_choice\"] == row[\"output_label\"])\n",
    "\n",
    "            row[\"know_choice\"] = get_choice(know_pred)\n",
    "            row[\"know_pred_is_correct\"] = int(row[\"know_choice\"] == row[\"output_label\"])\n",
    "\n",
    "        else:\n",
    "            row[\"baseline_is_correct\"] = int(baseline_pred.lower().strip() == row[\"output_text\"].lower().strip())\n",
    "            row[\"know_pred_is_correct\"] = int(know_pred.lower().strip() == row[\"output_text\"].lower().strip())\n",
    "        updated_rows.append(row)\n",
    "\n",
    "    # test_df = pd.DataFrame(updated_rows)\n",
    "    # test_df.to_csv(f\"{log_path}/{task}-results-2.csv\", index=False)\n",
    "\n",
    "    # with open(f\"{log_path}/overall_results.txt\", \"a\") as f:\n",
    "    #     f.write(f\"Task: {task}\\n\")\n",
    "    #     f.write(f\"Baseline Accuracy: {test_df['baseline_is_correct'].mean()}\\n\")\n",
    "    #     f.write(f\"Knowledge-Augmented Accuracy: {test_df['know_pred_is_correct'].mean()}\\n\")\n",
    "    #     f.write(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseline_is_correct     0.40\n",
       "know_pred_is_correct    0.45\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(updated_rows)\n",
    "df[[\"baseline_is_correct\", \"know_pred_is_correct\"]].mean().round(2)\n",
    "\n",
    "ropes_test = pd.read_csv(\"data/calm-bench/test-sets/ropes.csv\")\n",
    "ropes_test = ropes_test[[\"input\", \"domain_label\", \"question_type\"]]\n",
    "\n",
    "df = pd.concat([df, ropes_test], axis=1, join=\"inner\")\n",
    "df = df[[\"task\",  \"domain_label\", \"question_type\", \"input\", \"output_text\", \"know_pred\", \"know_pred_is_correct\", \"baseline_is_correct\", \"baseline_pred\"]]\n",
    "\n",
    "df.sample(100, random_state=42)[[\"baseline_is_correct\", \"know_pred_is_correct\"]].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baseline_is_correct     0.54\n",
       "know_pred_is_correct    0.65\n",
       "dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.read_csv(\"experiment_results/gpt-3.5/sample.csv\")\n",
    "s[[\"baseline_is_correct\", \"know_pred_is_correct\"]].mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiqa Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from datasets import load_from_disk\n",
    "from tqdm.notebook import tqdm \n",
    "from chromadb.utils import embedding_functions\n",
    "import pandas as pd \n",
    "from transformers.utils import logging\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"kb-cache/\")\n",
    "embedder = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"multi-qa-mpnet-base-dot-v1\"\n",
    ")\n",
    "db = client.get_collection(\"causal-kb\", embedding_function=embedder)\n",
    "\n",
    "log_path = \"experiment_results/phi-2\"\n",
    "\n",
    "test = pd.read_csv(\"data/calm-bench/test-sets/wiqa.csv\")\n",
    "\n",
    "\n",
    "updated_rows = []\n",
    "for i, row in tqdm(test.iterrows(), total=len(test)):\n",
    "    \n",
    "    if row[\"updated_answer\"] == \"more\":\n",
    "        gold_choice = \"a\"\n",
    "    elif row[\"updated_answer\"] == \"less\":\n",
    "        gold_choice = \"b\"\n",
    "    else:\n",
    "        gold_choice = \"c\"\n",
    "    \n",
    "    steps = literal_eval(row[\"question_para_step\"])\n",
    "    context = [\". \".join(tup) for tup in zip(np.arange(1, len(steps)+1).astype(str), steps) ]\n",
    "    context = \"\\n\".join(context)\n",
    "\n",
    "    example = f\"Context:\\n{context}\\nQuestion: {row['question']}\\nOptions: a) more b) less c) no effect\"\n",
    "    \n",
    "    #baseline_pred =  engine.extract_output(engine.query(example))\n",
    "    baseline_pred =  engine.query(example)\n",
    "    row[\"baseline_pred\"] = baseline_pred\n",
    "    row[\"baseline_choice\"] = get_choice(baseline_pred)\n",
    "    row[\"baseline_is_correct\"] = int(row[\"baseline_choice\"] == gold_choice)\n",
    "\n",
    "\n",
    "    # Generate knowledge-augmented prediction\n",
    "    top_context = db.query(query_texts=[example], n_results=1)[\"documents\"][0][0]\n",
    "    prompt_with_context = f\"{top_context}\\n{example}\"\n",
    "    #know_pred = engine.extract_output(engine.query(prompt_with_context))\n",
    "    know_pred = engine.query(prompt_with_context)\n",
    "    row[\"know_pred\"] = know_pred\n",
    "    row[\"know_choice\"] = get_choice(know_pred)\n",
    "    row[\"know_is_correct\"] = int(row[\"know_choice\"] == gold_choice)\n",
    "\n",
    "    updated_rows.append(row)   \n",
    "    \n",
    "# test_df = pd.DataFrame(updated_rows)\n",
    "# test_df.to_csv(f\"{log_path}/{task}-results.csv\", index=False)\n",
    "\n",
    "# with open(f\"{log_path}/overall_results.txt\", \"a\") as f:\n",
    "#     f.write(f\"Task: {task}\\n\")\n",
    "#     f.write(f\"Baseline Accuracy: {test_df['baseline_is_correct'].mean()}\\n\")\n",
    "#     f.write(f\"Knowledge-Augmented Accuracy: {test_df['know_pred_is_correct'].mean()}\\n\")\n",
    "#     f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(updated_rows)\n",
    "\n",
    "test_df[[\"baseline_is_correct\", \"know_is_correct\"]].mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(f\"{log_path}/wiqa-results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21328172743939702"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(901 + 146) / (3868 + 1041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
